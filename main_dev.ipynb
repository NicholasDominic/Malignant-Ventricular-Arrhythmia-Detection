{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "206c5fc2-54ff-4bd9-85dc-5c969b89dbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wfdb, wfdb.processing, heartpy\n",
    "from biosppy.signals import ecg\n",
    "from numpy import ndarray\n",
    "from pandas import read_csv as rcsv\n",
    "\n",
    "import json, re\n",
    "from time import time as t\n",
    "from datetime import datetime as dt\n",
    "\n",
    "from preproc_wfdb import show_ann_label, get_db, extract_ann, extract_rdheader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99235b37-b0c1-4961-8b21-341ac62d473a",
   "metadata": {},
   "source": [
    "EDA on VFDB: https://www.kaggle.com/code/kooaslansefat/eda-on-mit-bih-malignant-ventricular-fibrill-db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a557bc24-49cf-4b23-bd4e-770281cc4ed8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wfdb.io.show_ann_labels()\n",
    "# wfdb.io.show_ann_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92d9637e-ff2d-4838-a394-723fe25f584a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_dataset = rcsv(\"sample_p10_sensor_outputs.csv\")\n",
    "# (6000 / sample_dataset[\"HR (bpm)\"]).tolist() # 6000 (in miliseconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2df44284-0edc-442f-bffa-a27613522233",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_db_info(key : str, label : int, *args, **kwargs) -> dict:\n",
    "    \"\"\"\n",
    "    Available DB key options:\n",
    "        * key=\"vfdb\" for MIT-BIH Malignant Ventricular Ectopy Database\n",
    "        * key=\"nsrdb\" for MIT-BIH Normal Sinus Rhythm Database\n",
    "    \"\"\"\n",
    "    \n",
    "    dataset = get_db(key=key, verbose=False)\n",
    "    \n",
    "    try:\n",
    "        header = rcsv(\"data/{}-rdheader.csv\".format(key))\n",
    "    except:\n",
    "        raise FileNotFoundError(\n",
    "            \"Please call wf_preproc.extract_rdheader() and \" \\\n",
    "            \"then name your file with data/{}-rdheader.csv\".format(key)\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"db\" : dataset[\"db\"],\n",
    "        \"records\" : dataset[\"records\"],\n",
    "        \"header\" : header,\n",
    "        \"label\" : label\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57cb4215-5d18-4627-bd08-a1950fee06d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations = rcsv(\"data/vfdb-rdann.csv\")\n",
    "\n",
    "# # preprocess the annotation label\n",
    "# annotations.annot_aux = annotations.annot_aux.apply(\n",
    "#     lambda x: x.replace('(', '').replace('NSR', 'N').replace('VFIB', 'VF')\n",
    "# )\n",
    "\n",
    "# # set annotations index \"from\" and \"to\"\n",
    "# annotations.rename(columns={\"annot_idx\" : \"annot_idx_from\"}, inplace=True)\n",
    "# annotations.insert(\n",
    "#     column = \"annot_idx_to\",\n",
    "#     loc = len(annotations.columns)-1,\n",
    "#     value = annotations.groupby('r_name').annot_idx_from.shift(periods = -1, fill_value = -1).astype(int)\n",
    "# )\n",
    "\n",
    "# # remove last row from each r_name\n",
    "# annotations = annotations[annotations.annot_idx_to != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d145c024-e1e7-4ff1-997f-f2f3fe5a8423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = wfdb.rdrecord(records[0], channels=[0, 1], pn_dir=db)\n",
    "# a = wfdb.rdann(record_name=records[0], pn_dir=db, extension='atr')\n",
    "# a.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87ce08c-c9ba-462b-9438-84a354a543a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wfdb.plot_wfdb(record=r, annotation=a, time_units=\"samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0fcc958-4464-452d-8551-12eaad161686",
   "metadata": {},
   "source": [
    "* remember: f = N/t, where f = sampling frequency, N = signal length, t = time\n",
    "* time_per_second = signal_len / sampling_freq = 525000/250 = 2100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310ff97d-83d1-42ab-a898-4e2f9571b9de",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# record_num = 418\n",
    "# for _, i in annotations[annotations.r_name == record_num].iterrows():\n",
    "#     if _ > 50:\n",
    "#         FS = header[header.r_name == record_num].sampling_freq.squeeze()\n",
    "#         from_idx = i.annot_idx_from\n",
    "#         to_idx = i.annot_idx_to\n",
    "\n",
    "#         ann = wfdb.rdann(record_name=str(record_num), sampfrom=from_idx, sampto=to_idx+FS, extension=\"atr\", pn_dir=db)\n",
    "#         print(ann.sample)\n",
    "#         rr_interval = wfdb.processing.calc_rr(ann.sample, fs=FS)\n",
    "#         rr_interval = np.insert(rr_interval, 0, ann.sample[0]) if from_idx == 1 else rr_interval\n",
    "#             # np.insert() is used to add the first annotation sample (i.e., ann.sample[0]), only when annot_idx = 1\n",
    "#         rr_interval = rr_interval # / ann.fs # normalize\n",
    "#         print(\"f={}, t={} {}\".format(from_idx, to_idx, rr_interval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a45e2938-169c-4ca8-84eb-8a55b2a90937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_record_start_partitions(\n",
    "    record : str,\n",
    "    db : str,\n",
    "    min_duration : int,\n",
    "    freq_rate : int,\n",
    "    resampling : int = 0,\n",
    "    max_total_partition : int = 10,\n",
    "    *args, **kwargs\n",
    ") -> dict:\n",
    "    \n",
    "    def get_label(annotation):\n",
    "        p = re.compile(\"([A-Za-z]+)\")\n",
    "        return p.search(annotation)[1]\n",
    "    \n",
    "    start_sample = (min_duration * 60) * freq_rate # freq = N / time(s), therefore N = freq x time\n",
    "    annotation = wfdb.rdann(record, 'atr', pn_dir=db, sampfrom=start_sample)\n",
    "    signals, _ = wfdb.rdsamp(record, pn_dir=db, sampfrom=start_sample)\n",
    "    \n",
    "    if resampling != 0:\n",
    "        start_sample = (min_duration * 60) * resampling\n",
    "        signals, annotation = wfdb.processing.resample_multichan(\n",
    "            signals, annotation, freq_rate, resampling)\n",
    "    \n",
    "    results = []\n",
    "    if db.lower() == \"vfdb\":\n",
    "        annotations = [get_label(a) for a in annotation.aux_note]\n",
    "        annotations = list(map(lambda x: x.replace('NSR', 'N').replace('VFIB', 'VF'), annotations))\n",
    "        positive_labels = [\"VT\", \"VF\", \"VFL\"]\n",
    "        \n",
    "        for ann, annot_sample in zip(annotations, annotation.sample):\n",
    "            if ann in positive_labels:\n",
    "                results.append(annot_sample-start_sample) # n sample before events\n",
    "    \n",
    "    elif db.lower() == \"nsrdb\":\n",
    "        nsrdb_start_partition = 0\n",
    "        \n",
    "        for i in range(max_total_partition):\n",
    "            results.append(nsrdb_start_partition)\n",
    "            nsrdb_start_partition += start_sample\n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Available DB: 'vfdb' and 'nsrdb' only.\")\n",
    "    \n",
    "    results = [r for r in results if r >= 0] # remove non-negative values\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b6dbf8-72c8-4a86-8c33-2b4717f7d9e8",
   "metadata": {},
   "source": [
    "### Preprocessing (R-R Interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13a0a706-d111-4685-b365-9d611207b564",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dotdict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a9361c9-defa-4541-9fa2-53bc9f54f906",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalPreprocessing():\n",
    "    def __init__(self, record_idx : str, db : str, *args, **kwargs) -> None:\n",
    "        super(SignalPreprocessing, self).__init__()\n",
    "        self.record_idx = record_idx\n",
    "        self.db = db\n",
    "        \n",
    "        # record\n",
    "        self.r = wfdb.rdrecord(record_name=self.record_idx, pn_dir=self.db)\n",
    "        \n",
    "        # annotation\n",
    "        self.a = wfdb.rdann(record_name=self.record_idx, pn_dir=self.db, extension='atr')\n",
    "        \n",
    "        self.signals = self.r.p_signal # can be multi-dimensional channels\n",
    "        self.channels = self.r.sig_name\n",
    "        self.freq_rate = self.r.fs\n",
    "        \n",
    "    def extract_rpeaks(self, signal, *args, **kwargs) -> ndarray:\n",
    "        # segment\n",
    "        rpeaks, = ecg.engzee_segmenter(\n",
    "            signal=signal,\n",
    "            sampling_rate=self.freq_rate\n",
    "        )\n",
    "\n",
    "        # correct R-peak locations\n",
    "        rpeaks, = ecg.correct_rpeaks(\n",
    "            signal=signal, rpeaks=rpeaks,\n",
    "            sampling_rate=self.freq_rate, tol=.05\n",
    "        )\n",
    "\n",
    "        # extract templates\n",
    "        _, rpeaks = ecg.extract_heartbeats(\n",
    "            signal=signal, rpeaks=rpeaks,\n",
    "            sampling_rate=self.freq_rate, before=.2, after=.4\n",
    "        )\n",
    "        \n",
    "        return rpeaks\n",
    "        \n",
    "    def filter_signal(self, signal, low_freq, high_freq, *args, **kwargs) -> ndarray:\n",
    "        return heartpy.filter_signal(\n",
    "            signal, filtertype='bandpass',\n",
    "            cutoff=[low_freq, high_freq], sample_rate=self.freq_rate\n",
    "        )\n",
    "    \n",
    "    def get_peaklist(self,\n",
    "        start_partitions : list,\n",
    "        duration : int,\n",
    "        label : int,\n",
    "        resampling : int = 0,\n",
    "        *args, **kwargs\n",
    "    ) -> dict:\n",
    "        '''\n",
    "        Example return:\n",
    "        return_example = {\n",
    "            \"peaks\" : {\n",
    "                1000 : [\n",
    "                    {\"channel\" : \"ECG1\", \"value\" : [20, 30, 40]},\n",
    "                    {\"channel\" : \"ECG2\", \"value\" : [50, 60, 70]},\n",
    "                ],\n",
    "                2500 : [\n",
    "                    {\"channel\" : \"ECG1\", \"value\" : [55, 66, 77]},\n",
    "                    {\"channel\" : \"ECG2\", \"value\" : [75, 85, 96]},\n",
    "                ]\n",
    "            }, \n",
    "            \"created_at\" : \"2024-02-15\", \n",
    "            \"exc_time\" : 50.184\n",
    "        }\n",
    "        '''\n",
    "        \n",
    "        # to calculate exc. time (in seconds)\n",
    "        start_time = t()\n",
    "        \n",
    "        # reassign variables since they can be overrided if resampling != 0\n",
    "        fr = self.freq_rate\n",
    "        signals = self.signals\n",
    "        \n",
    "        # resampling signal (if any)\n",
    "        if resampling != 0:\n",
    "            resampled_signals, _ = wfdb.processing.resample_multichan(\n",
    "                self.signals, self.a, fr, resampling\n",
    "            )\n",
    "            fr = resampling\n",
    "            signals = resampled_signals\n",
    "            \n",
    "            # if you did resampling, partitions index should be adjusted\n",
    "            # based on the newest max. signal length.\n",
    "            start_partitions = [i for i in start_partitions if i <= len(signals)]\n",
    "        \n",
    "        FINAL_RESULTS, PARTITION = {}, {}\n",
    "        for start_partition in start_partitions:\n",
    "            delta = (duration * 60) * fr # remember, freq = N / time(s), therefore N = freq x time\n",
    "            curr_signals = signals[start_partition:start_partition+delta, :]\n",
    "            \n",
    "            PEAKS_CHANNEL = []\n",
    "            for i, channel in enumerate(self.channels):\n",
    "                signal = curr_signals[:, i]\n",
    "                signal = self.filter_signal(signal, 7, 30) # filtered\n",
    "                peaks = self.extract_rpeaks(signal)\n",
    "                peaks = [int(p) for p in peaks] # convert from np.int32 to INT\n",
    "                PEAKS_CHANNEL.append(dotdict({\"channel\" : channel, \"value\" : peaks}))\n",
    "            PARTITION[int(start_partition)] = PEAKS_CHANNEL\n",
    "        \n",
    "        FINAL_RESULTS[\"peaks\"] = PARTITION\n",
    "        FINAL_RESULTS[\"label\"] = label\n",
    "        FINAL_RESULTS[\"exc_time\"] = round(t()-start_time, 3)\n",
    "        FINAL_RESULTS[\"created_at\"] = dt.now().strftime(\"%Y-%m-%d %X\")\n",
    "        \n",
    "        return dotdict(FINAL_RESULTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a27476d7-2e4a-48ef-87c0-e8cf25e39851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483bfab4-43cf-421a-86cf-38ab0e5fd954",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dataset(verbose : bool = True, *args, **kwargs) -> None:\n",
    "    db = kwargs[\"db\"]\n",
    "    records = kwargs[\"records\"]\n",
    "    label = kwargs[\"label\"]\n",
    "    \n",
    "    for r in records:\n",
    "        st = get_record_start_partitions(record=r, db=db, min_duration=5, freq_rate=128)\n",
    "        proc = SignalPreprocessing(record_idx=r, db=db)\n",
    "        peaklist_result = proc.get_peaklist(start_partitions=st, duration=5, label=label)\n",
    "        \n",
    "        PATH = \"data/{}\".format(db)\n",
    "        if not os.path.exists(PATH):\n",
    "            os.mkdir(PATH)\n",
    "        \n",
    "        with open(\"data/{}/{}.json\".format(db, r), \"w\") as outfile: \n",
    "            json.dump(peaklist_result, outfile)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"Record {} (label={}) was completed.\".format(r, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1bb877a3-6c32-417b-800e-eb67678bbc63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Record 16265 (label=0) was completed.\n",
      "Record 16272 (label=0) was completed.\n",
      "Record 16273 (label=0) was completed.\n",
      "Record 16420 (label=0) was completed.\n",
      "Record 16483 (label=0) was completed.\n",
      "Record 16539 (label=0) was completed.\n",
      "Record 16773 (label=0) was completed.\n",
      "Record 16786 (label=0) was completed.\n",
      "Record 16795 (label=0) was completed.\n",
      "Record 17052 (label=0) was completed.\n",
      "Record 17453 (label=0) was completed.\n",
      "Record 18177 (label=0) was completed.\n",
      "Record 18184 (label=0) was completed.\n",
      "Record 19088 (label=0) was completed.\n",
      "Record 19090 (label=0) was completed.\n",
      "Record 19093 (label=0) was completed.\n",
      "Record 19140 (label=0) was completed.\n",
      "Record 19830 (label=0) was completed.\n"
     ]
    }
   ],
   "source": [
    "create_dataset(**call_db_info(\"nsrdb\", 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73140882-79be-4e91-b2aa-ccaf6bd0b62b",
   "metadata": {},
   "source": [
    "### Read files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efa16eac-3e4b-40ea-ad70-f0fb663e49c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from heartpy import analysis\n",
    "import hrvanalysis as hrva\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d3475cda-42ad-4158-b179-353340851a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_preproc(rr_interval : list) -> list:\n",
    "    nn_interval = hrva.remove_outliers(rr_intervals=rr_interval, verbose=False)\n",
    "\n",
    "    # @param method: \"malik\", \"kamath\", \"karlsson\", \"acar\"\n",
    "    nn_interval = hrva.remove_ectopic_beats(rr_intervals=nn_interval, method=\"malik\", verbose=False)\n",
    "\n",
    "    # @param interpolation_method: 'linear', 'time', 'index', 'values', 'nearest', 'zero', 'slinear',\n",
    "    # 'quadratic', 'cubic', 'barycentric', 'krogh', 'spline', 'polynomial', 'from_derivatives',\n",
    "    # 'piecewise_polynomial', 'pchip', 'akima', 'cubicspline'\n",
    "    nn_interval = hrva.interpolate_nan_values(rr_intervals=nn_interval, interpolation_method=\"cubic\")\n",
    "\n",
    "    # remove NaN values which weren't filtered during interpolation; e.g., in the last index\n",
    "    nn_interval = [i for i in nn_interval if str(i) != \"nan\"]\n",
    "    \n",
    "    return nn_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "02dec9d9-05b8-4a54-9c46-13d6d511755b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "DF_RECORDS = []\n",
    "dataset_pos_filepath = [\"data/_vfdb/{}\".format(p) for p in os.listdir(\"data/_vfdb\") if p.endswith(\".json\")]\n",
    "dataset_neg_filepath = [\"data/_nsrdb/{}\".format(p) for p in os.listdir(\"data/_nsrdb\") if p.endswith(\".json\")]\n",
    "\n",
    "for filepath in (dataset_pos_filepath+dataset_neg_filepath): # LOOP per file\n",
    "    with open(filepath) as file:\n",
    "        data = json.load(fp=file)\n",
    "\n",
    "    for idx in data[\"peaks\"].keys(): # LOOP per start_partition\n",
    "        for ch_num, ch in enumerate(data[\"peaks\"][idx]): # LOOP per channel\n",
    "            peaklist = ch[\"value\"]\n",
    "            \n",
    "            try:\n",
    "                rr = analysis.calc_rr(peaklist=peaklist, sample_rate=128)\n",
    "                nn_interval = rr_preproc(rr_interval=rr[\"RR_list\"])\n",
    "\n",
    "                FEATURES = {\n",
    "                    \"record_id\" : int(filepath.split(\"/\")[-1].split(\".json\")[0]),\n",
    "                    \"start_partition_idx\" : idx,\n",
    "                    \"channel\" : ch[\"channel\"] + \"_{}\".format(str(ch_num))\n",
    "                }\n",
    "\n",
    "                # Reference:\n",
    "                # 1. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5624990/\n",
    "                # 2. https://aura-healthcare.github.io/hrv-analysis/hrvanalysis.html\n",
    "\n",
    "                # TIME DOMAIN\n",
    "                ftr_time_domain = hrva.get_time_domain_features(nn_interval)\n",
    "                FEATURES.update(ftr_time_domain)\n",
    "\n",
    "                ftr_geometric_time_domain = hrva.get_geometrical_features(nn_interval)\n",
    "                FEATURES.update(ftr_geometric_time_domain)\n",
    "\n",
    "                # Frequency Domain\n",
    "                ftr_freq_domain = hrva.get_frequency_domain_features(nn_interval)\n",
    "                FEATURES.update(ftr_freq_domain)\n",
    "\n",
    "                # Non-linear Domain\n",
    "                ftr_entropy = hrva.get_sampen(nn_interval) # sample entropy\n",
    "                FEATURES.update({\"entropy\" : ftr_entropy[\"sampen\"]})\n",
    "\n",
    "                ftr_poincare = hrva.get_poincare_plot_features(nn_interval)\n",
    "                FEATURES.update(ftr_poincare)\n",
    "\n",
    "                # CVI (Cardiac Sympathetic Index), CSI (Cardiac Vagal Index)\n",
    "                ftr_csi_cvi = hrva.get_csi_cvi_features(nn_interval)\n",
    "                FEATURES.update(ftr_csi_cvi)\n",
    "\n",
    "                FEATURES.update({\"label\" : int(data[\"label\"])}) # set label\n",
    "                DF_RECORDS.append(FEATURES)\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(\"File: {}\\nError: {}\".format(filepath, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "b1dec8bc-ae00-49f0-b092-efd921fd0991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame as df, read_csv as rcsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "feceb723-5f35-47c1-b5d1-e6491c4e7aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.preprocessing import MinMaxScaler as mms\n",
    "from sklearn.metrics import accuracy_score, auc, roc_curve, confusion_matrix as cfmat\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68ed8f1d-782a-4ea1-8eb4-7e398c6fd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vfdb_dataset = df(DF_RECORDS)\n",
    "# vfdb_dataset.to_csv(\"data/features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c0934760-ce49-473a-8029-963e88cbd187",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    THIS_TIMESTAMP = \"20231022\"\n",
    "    THIS_MODEL = \"mlp\"\n",
    "    vfdb_dataset = rcsv(\"feature_store/{}_features.csv\".format(THIS_TIMESTAMP))\n",
    "    \n",
    "    # set random state (seed)\n",
    "    RAND_SEED = 43\n",
    "    \n",
    "    # feature selection\n",
    "    X = vfdb_dataset[['mean_nni', 'rmssd', 'mean_hr', 'triangular_index', \\\n",
    "                      'total_power', 'csi', 'cvi', 'entropy']]\n",
    "    y = vfdb_dataset.label\n",
    "    \n",
    "    # split dataset\n",
    "    X_train, X_test, y_train, y_test = tts(X, y, test_size=.3, random_state=RAND_SEED)\n",
    "    \n",
    "    # scale the data\n",
    "    scaler = mms()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "86f23797-6b64-4672-a0e1-5989f36705c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_pred : ndarray, y_test : ndarray, *args, **kwargs) -> dict:\n",
    "    start_time = t()\n",
    "    \n",
    "    tn, fp, fn, tp = cfmat(y_pred, y_test).ravel()\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * precision * recall) / (precision + recall)\n",
    "    fpr, tpr, threshold = roc_curve(y_test, y_pred)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\" : float(accuracy_score(y_pred, y_test)),\n",
    "        \"true_pos\" : int(tp),\n",
    "        \"true_neg\" : int(tn),\n",
    "        \"false_pos\" : int(fp),\n",
    "        \"false_neg\" : int(fn),\n",
    "        \"recall\" : float(recall),\n",
    "        \"precision\" : float(precision),\n",
    "        \"f1_score\" : float(f1_score),\n",
    "        \"false_pos_rate\" : fpr.tolist(),\n",
    "        \"true_pos_rate\" : tpr.tolist(),\n",
    "        \"auc_roc_threshold\" : threshold.tolist(),\n",
    "        \"auc_roc_score\" : float(auc(fpr, tpr)),\n",
    "        \"exc_time\" : round(t()-start_time, 3),\n",
    "        \"created_at\" : dt.now().strftime(\"%Y-%m-%d %X\")\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "16e41e59-489a-4141-b0cf-459681c4d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cpuinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "379f9ef5-7ab3-452c-8b0c-3531d17b1baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selections = {\n",
    "    \"lr\" : LR(random_state=RAND_SEED),\n",
    "    \"rf\" : RFC(random_state=RAND_SEED),\n",
    "    \"svm\" : SVC(random_state=RAND_SEED),\n",
    "    \"mlp\" : MLPClassifier(\n",
    "        hidden_layer_sizes=(16, 4),\n",
    "        batch_size=64,\n",
    "        learning_rate_init=1e-2,\n",
    "        early_stopping=True,\n",
    "        random_state=RAND_SEED\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "594c349b-eb1d-4b21-b94f-9c7e43e19ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_training_time = t()\n",
    "model_id = str(int(t())) + \"-{}\".format(THIS_MODEL)\n",
    "model = model_selections[THIS_MODEL]\n",
    "model.fit(X_train_scaled, y_train)\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "model_training_log = {\n",
    "    \"model_id\" : model_id,\n",
    "    \"model_name\" : THIS_MODEL,\n",
    "    \"model_version\" : \"1.0\",\n",
    "    \"cloud_storage_uri\" : \"gs://\",\n",
    "    \"evaluation\" : evaluate(y_pred, y_test),\n",
    "    \"device_type\" : \"cpu\",\n",
    "    \"device_name\" : cpuinfo.get_cpu_info()['brand_raw'],\n",
    "    \"device_count\" : int(1),\n",
    "    \"exc_time_sec\" : t()-start_training_time,\n",
    "    \"data_snapshot_dt\" : THIS_TIMESTAMP,\n",
    "    \"prc_dt\" : dt.now().strftime(\"%Y-%m-%d %X\")\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "44b14760-69a0-4e76-bb46-4d31bae7772a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.load(open(filepath, 'rb')).predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855d9c9b-10f1-4c45-860f-da11007913d7",
   "metadata": {},
   "source": [
    "### Test with PyTorch (for neural nets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85802257-6c89-4bc3-a4d6-26b8bbd3a8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c458526b-61fe-4825-8b40-cddcf6bde668",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralClassifier(torch.nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(NeuralClassifier,self).__init__()\n",
    "        self.input_layer = torch.nn.Linear(input_dim, 256)\n",
    "        self.hidden_layer1 = torch.nn.Linear(256, 128)\n",
    "        self.hidden_layer2 = torch.nn.Linear(128, 64)\n",
    "        self.hidden_layer3 = torch.nn.Linear(64, 32)\n",
    "        self.output_layer = torch.nn.Linear(32, output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        out = self.relu(self.hidden_layer1(out))\n",
    "        out = self.relu(self.hidden_layer2(out))\n",
    "        out = self.relu(self.hidden_layer3(out))\n",
    "        out = self.output_layer(out)\n",
    "        return self.sigmoid(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8a003b07-3432-482b-b7a9-7e3a82dca538",
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.tensor(x, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.length = self.x.shape[0]\n",
    " \n",
    "    def __getitem__(self,idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fba50056-875e-43c4-b7f2-28e3bc03f6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = NeuralClassifier(input_dim=len(X.columns), output_dim=1)\n",
    "model = model.to(device)\n",
    "\n",
    "# set hyperparams\n",
    "lr = 1e-5\n",
    "bs = 64\n",
    "num_epochs = 1000\n",
    "criterion = torch.nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7f959147-a38f-44ab-b9a6-61583f28668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.FloatTensor(X_train_scaled).to(device)\n",
    "X_test = torch.FloatTensor(X_test_scaled).to(device)\n",
    "y_train = torch.FloatTensor(y_train.tolist()).to(device)\n",
    "y_test = torch.FloatTensor(y_test.tolist()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a4189d72-9929-4ead-91b5-35c59c97ce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = dataset(X_train_scaled, y_train.tolist())\n",
    "trainloader = DataLoader(trainset, batch_size=bs, shuffle=True)\n",
    "\n",
    "testset = dataset(X_test_scaled, y_test.tolist())\n",
    "testloader = DataLoader(testset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "cc66ceea-1012-4fa1-9f52-33fe19c259a4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/1000, Train Loss: 0.6773, Test Loss: 0.6696\n",
      "Train acc: 0.641, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6752, Test Loss: 0.6695\n",
      "Train acc: 0.781, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6758, Test Loss: 0.6695\n",
      "Train acc: 0.844, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6750, Test Loss: 0.6694\n",
      "Train acc: 0.766, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6755, Test Loss: 0.6693\n",
      "Train acc: 0.750, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6712, Test Loss: 0.6692\n",
      "Train acc: 0.766, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6765, Test Loss: 0.6692\n",
      "Train acc: 0.750, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6741, Test Loss: 0.6691\n",
      "Train acc: 0.797, Test acc: 0.805\n",
      "\n",
      "Epoch 50/1000, Train Loss: 0.6601, Test Loss: 0.6690\n",
      "Train acc: 1.000, Test acc: 0.805\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6383, Test Loss: 0.6125\n",
      "Train acc: 0.812, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6228, Test Loss: 0.6123\n",
      "Train acc: 0.875, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6222, Test Loss: 0.6121\n",
      "Train acc: 0.891, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6215, Test Loss: 0.6119\n",
      "Train acc: 0.906, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6342, Test Loss: 0.6117\n",
      "Train acc: 0.828, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6270, Test Loss: 0.6115\n",
      "Train acc: 0.844, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6359, Test Loss: 0.6113\n",
      "Train acc: 0.797, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6340, Test Loss: 0.6111\n",
      "Train acc: 0.844, Test acc: 0.842\n",
      "\n",
      "Epoch 100/1000, Train Loss: 0.6230, Test Loss: 0.6110\n",
      "Train acc: 1.000, Test acc: 0.837\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5203, Test Loss: 0.4888\n",
      "Train acc: 0.906, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5237, Test Loss: 0.4886\n",
      "Train acc: 0.906, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5262, Test Loss: 0.4883\n",
      "Train acc: 0.891, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5375, Test Loss: 0.4880\n",
      "Train acc: 0.859, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5120, Test Loss: 0.4877\n",
      "Train acc: 0.828, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5567, Test Loss: 0.4875\n",
      "Train acc: 0.766, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5334, Test Loss: 0.4872\n",
      "Train acc: 0.891, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.5288, Test Loss: 0.4869\n",
      "Train acc: 0.891, Test acc: 0.878\n",
      "\n",
      "Epoch 150/1000, Train Loss: 0.3437, Test Loss: 0.4866\n",
      "Train acc: 1.000, Test acc: 0.878\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4206, Test Loss: 0.3775\n",
      "Train acc: 0.906, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.3858, Test Loss: 0.3773\n",
      "Train acc: 0.938, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4249, Test Loss: 0.3771\n",
      "Train acc: 0.906, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4636, Test Loss: 0.3769\n",
      "Train acc: 0.938, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4070, Test Loss: 0.3767\n",
      "Train acc: 0.906, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.3831, Test Loss: 0.3765\n",
      "Train acc: 0.922, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4135, Test Loss: 0.3763\n",
      "Train acc: 0.906, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.5267, Test Loss: 0.3760\n",
      "Train acc: 0.797, Test acc: 0.919\n",
      "\n",
      "Epoch 200/1000, Train Loss: 0.4043, Test Loss: 0.3758\n",
      "Train acc: 0.667, Test acc: 0.919\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3375, Test Loss: 0.2963\n",
      "Train acc: 0.938, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3341, Test Loss: 0.2962\n",
      "Train acc: 0.984, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3688, Test Loss: 0.2960\n",
      "Train acc: 0.969, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3463, Test Loss: 0.2959\n",
      "Train acc: 0.953, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3383, Test Loss: 0.2957\n",
      "Train acc: 0.984, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.2966, Test Loss: 0.2956\n",
      "Train acc: 0.953, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3720, Test Loss: 0.2954\n",
      "Train acc: 0.922, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.3668, Test Loss: 0.2952\n",
      "Train acc: 0.969, Test acc: 0.946\n",
      "\n",
      "Epoch 250/1000, Train Loss: 0.1840, Test Loss: 0.2951\n",
      "Train acc: 1.000, Test acc: 0.946\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2618, Test Loss: 0.2308\n",
      "Train acc: 0.969, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2605, Test Loss: 0.2307\n",
      "Train acc: 0.984, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2599, Test Loss: 0.2306\n",
      "Train acc: 0.984, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2872, Test Loss: 0.2305\n",
      "Train acc: 0.969, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2516, Test Loss: 0.2304\n",
      "Train acc: 0.984, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2708, Test Loss: 0.2302\n",
      "Train acc: 0.953, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.3067, Test Loss: 0.2301\n",
      "Train acc: 0.922, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.2369, Test Loss: 0.2299\n",
      "Train acc: 0.984, Test acc: 0.968\n",
      "\n",
      "Epoch 300/1000, Train Loss: 0.3753, Test Loss: 0.2298\n",
      "Train acc: 1.000, Test acc: 0.968\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.2311, Test Loss: 0.1714\n",
      "Train acc: 0.953, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.2180, Test Loss: 0.1713\n",
      "Train acc: 0.969, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.1854, Test Loss: 0.1712\n",
      "Train acc: 0.984, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.2309, Test Loss: 0.1710\n",
      "Train acc: 0.922, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.1631, Test Loss: 0.1709\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.1601, Test Loss: 0.1708\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.1762, Test Loss: 0.1707\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.2121, Test Loss: 0.1706\n",
      "Train acc: 0.984, Test acc: 0.973\n",
      "\n",
      "Epoch 350/1000, Train Loss: 0.0963, Test Loss: 0.1705\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1044, Test Loss: 0.1206\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1342, Test Loss: 0.1204\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1964, Test Loss: 0.1203\n",
      "Train acc: 0.953, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1349, Test Loss: 0.1202\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1427, Test Loss: 0.1201\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1150, Test Loss: 0.1200\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1302, Test Loss: 0.1199\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1583, Test Loss: 0.1198\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 400/1000, Train Loss: 0.1315, Test Loss: 0.1197\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.1256, Test Loss: 0.0927\n",
      "Train acc: 0.953, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.1143, Test Loss: 0.0926\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.0814, Test Loss: 0.0926\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.1242, Test Loss: 0.0926\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.0860, Test Loss: 0.0925\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.0789, Test Loss: 0.0925\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.1015, Test Loss: 0.0925\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.0979, Test Loss: 0.0925\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 450/1000, Train Loss: 0.0752, Test Loss: 0.0924\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0680, Test Loss: 0.0734\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0998, Test Loss: 0.0734\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0625, Test Loss: 0.0733\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0434, Test Loss: 0.0733\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0834, Test Loss: 0.0733\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0642, Test Loss: 0.0733\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0830, Test Loss: 0.0733\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.1089, Test Loss: 0.0733\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 500/1000, Train Loss: 0.0262, Test Loss: 0.0733\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0538, Test Loss: 0.0640\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0618, Test Loss: 0.0640\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0844, Test Loss: 0.0640\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0445, Test Loss: 0.0639\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.1205, Test Loss: 0.0639\n",
      "Train acc: 0.953, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0386, Test Loss: 0.0639\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0308, Test Loss: 0.0639\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0546, Test Loss: 0.0639\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 550/1000, Train Loss: 0.0226, Test Loss: 0.0639\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0817, Test Loss: 0.0598\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0886, Test Loss: 0.0598\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0496, Test Loss: 0.0597\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0486, Test Loss: 0.0597\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0297, Test Loss: 0.0596\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0238, Test Loss: 0.0596\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0446, Test Loss: 0.0596\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0399, Test Loss: 0.0596\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 600/1000, Train Loss: 0.0340, Test Loss: 0.0596\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0648, Test Loss: 0.0570\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0476, Test Loss: 0.0571\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0969, Test Loss: 0.0571\n",
      "Train acc: 0.953, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0179, Test Loss: 0.0572\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0255, Test Loss: 0.0572\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0414, Test Loss: 0.0573\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0301, Test Loss: 0.0573\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0256, Test Loss: 0.0573\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 650/1000, Train Loss: 0.0161, Test Loss: 0.0574\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0224, Test Loss: 0.0565\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0142, Test Loss: 0.0561\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0703, Test Loss: 0.0558\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0498, Test Loss: 0.0554\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0219, Test Loss: 0.0552\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0617, Test Loss: 0.0549\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0450, Test Loss: 0.0547\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0248, Test Loss: 0.0545\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 700/1000, Train Loss: 0.0016, Test Loss: 0.0543\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0499, Test Loss: 0.0568\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0316, Test Loss: 0.0569\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0416, Test Loss: 0.0570\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0214, Test Loss: 0.0571\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0169, Test Loss: 0.0572\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0710, Test Loss: 0.0572\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0283, Test Loss: 0.0572\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0194, Test Loss: 0.0573\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 750/1000, Train Loss: 0.0030, Test Loss: 0.0574\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0539, Test Loss: 0.0588\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0286, Test Loss: 0.0588\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0322, Test Loss: 0.0588\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0328, Test Loss: 0.0588\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0212, Test Loss: 0.0587\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0168, Test Loss: 0.0587\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0484, Test Loss: 0.0587\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0220, Test Loss: 0.0587\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 800/1000, Train Loss: 0.0104, Test Loss: 0.0587\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0325, Test Loss: 0.0620\n",
      "Train acc: 0.984, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0220, Test Loss: 0.0620\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0112, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0340, Test Loss: 0.0621\n",
      "Train acc: 0.984, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0290, Test Loss: 0.0622\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0128, Test Loss: 0.0623\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0325, Test Loss: 0.0623\n",
      "Train acc: 0.984, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.0216, Test Loss: 0.0624\n",
      "Train acc: 1.000, Test acc: 0.973\n",
      "\n",
      "Epoch 850/1000, Train Loss: 0.8867, Test Loss: 0.0616\n",
      "Train acc: 0.667, Test acc: 0.973\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0823, Test Loss: 0.0621\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0206, Test Loss: 0.0622\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0142, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0060, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0349, Test Loss: 0.0621\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0309, Test Loss: 0.0621\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0224, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0113, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 900/1000, Train Loss: 0.0037, Test Loss: 0.0621\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0209, Test Loss: 0.0647\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0071, Test Loss: 0.0647\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0277, Test Loss: 0.0648\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0182, Test Loss: 0.0648\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0110, Test Loss: 0.0648\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0154, Test Loss: 0.0649\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0442, Test Loss: 0.0649\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0656, Test Loss: 0.0649\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 950/1000, Train Loss: 0.0026, Test Loss: 0.0649\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0164, Test Loss: 0.0729\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0165, Test Loss: 0.0729\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0858, Test Loss: 0.0729\n",
      "Train acc: 0.969, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0127, Test Loss: 0.0728\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0176, Test Loss: 0.0728\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0226, Test Loss: 0.0727\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0050, Test Loss: 0.0726\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0276, Test Loss: 0.0726\n",
      "Train acc: 0.984, Test acc: 0.977\n",
      "\n",
      "Epoch 1000/1000, Train Loss: 0.0020, Test Loss: 0.0725\n",
      "Train acc: 1.000, Test acc: 0.977\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for j, (X_train, y_train) in enumerate(trainloader):\n",
    "        X_train = X_train.to(device)\n",
    "        y_train = y_train.to(device)\n",
    "        \n",
    "        #clear out the gradients from the last step loss.backward()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #forward feed\n",
    "        output_train = model(X_train)\n",
    "        output_train = output_train.squeeze(1)\n",
    "\n",
    "        #calculate the loss\n",
    "        loss_train = criterion(output_train, y_train)\n",
    "\n",
    "        acc_train = (output_train.round() == y_train).float().mean().item()\n",
    "\n",
    "        #backward propagation: calculate gradients\n",
    "        loss_train.backward()\n",
    "\n",
    "        #update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        output_test = model(X_test)\n",
    "        output_test = output_test.squeeze(1)\n",
    "\n",
    "        loss_test = criterion(output_test, y_test)\n",
    "        acc_test = (output_test.round() == y_test).float().mean().item()\n",
    "\n",
    "        # train_losses[epoch] = loss_train.item()\n",
    "        # test_losses[epoch] = loss_test.item()\n",
    "\n",
    "        if (epoch + 1) % 50 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss_train.item():.4f}, Test Loss: {loss_test.item():.4f}\")\n",
    "            print(\"Train acc: {:.3f}, Test acc: {:.3f}\".format(acc_train, acc_test))\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9fbf96-8f28-4d32-b5eb-2303334c414c",
   "metadata": {},
   "source": [
    "### Poincare and PSD plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f02731d-d5c2-4001-9590-05120f6808fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hrvanalysis import plot_psd, plot_poincare\n",
    "\n",
    "# plot_poincare(nn_interval, plot_sd_features=True)\n",
    "# plot_psd(nn_interval)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mit-bih-venv",
   "language": "python",
   "name": "mit-bih-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
