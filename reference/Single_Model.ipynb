{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FP5qHUL2q_FM"
   },
   "source": [
    "# Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6qDs7INvp95"
   },
   "source": [
    "## Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VZ7O-DUdwOcH"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tf6MZPm8vtVi"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dB_qVdoEwO5l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4akNn-Vvodl"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VJIdxvHNqi4J"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC as SVM\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uCvXgx5Yxj40"
   },
   "outputs": [],
   "source": [
    "!pip install neupy\n",
    "from neupy.algorithms import PNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w8KzE7I_6mIS"
   },
   "source": [
    "## K * L Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-90ZKyFyqyzH"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone as clone_model\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2LhNokVwWk0"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwspQuYlAMem"
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51ja3EZ8wa81"
   },
   "outputs": [],
   "source": [
    "time_durations = [2,3,4,5]\n",
    "path = \"/content/drive/My Drive/Signal/\"\n",
    "folder = \"Dataset/\"\n",
    "datasets = []\n",
    "for time_duration in time_durations:\n",
    "  dataset = pd.read_csv(path + folder + \"Dataset_{}.csv\".format(time_duration))\n",
    "  dataset.drop(labels='File', axis=1, inplace=True)\n",
    "  datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vn11RF95Gvkh"
   },
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQ06atyVAP3l"
   },
   "outputs": [],
   "source": [
    "def feature_scaling(dataset):\n",
    "  result = []\n",
    "  scaler = MinMaxScaler()\n",
    "  columns = dataset.columns\n",
    "  dataset[columns[:-1]] = scaler.fit_transform(dataset[columns[:-1]])\n",
    "  result = dataset\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJYMcITvA-Pj"
   },
   "outputs": [],
   "source": [
    "scaled_datasets = []\n",
    "for dataset in datasets:\n",
    "  scaled_datasets.append(feature_scaling(dataset.copy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ebWKuZho18h"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_SbkNrwOIk6"
   },
   "source": [
    "## Save Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZBTkBp_1XtK"
   },
   "source": [
    "### CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xoSXfEGqOMgk"
   },
   "outputs": [],
   "source": [
    "def save_csv(df, folder, filename):\n",
    "  path = \"/content/drive/My Drive/Signal/Single Model/\" + folder + \"/\" + filename\n",
    "  df.to_csv(path, index=False)\n",
    "  print(\"{} saved!\".format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jPfjKZGO1ZQT"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sPYvCTSMOL_t"
   },
   "outputs": [],
   "source": [
    "def save_model(model, folder, filename):\n",
    "  path = \"/content/drive/My Drive/Signal/Single Model/\" + folder + \"/\"\n",
    "  with open(path+filename, 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "  print(\"Model {} saved!\".format(filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crDess4Nt2uN"
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n-MG2FlblIWr"
   },
   "outputs": [],
   "source": [
    "def get_metrics(true_n, false_p, false_n, true_p):\n",
    "  acc = (true_p + true_n) / (true_p + true_n + false_p + false_n)\n",
    "  prec = (true_p) / (true_p + false_p)\n",
    "  rec = (true_p) / (true_p + false_n)\n",
    "  f1 = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "  result = {\n",
    "      'accuracy': acc,\n",
    "      'precision': prec,\n",
    "      'recall': rec,\n",
    "      'f1_score': f1\n",
    "  }\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwCVImCBUmYe"
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PoaTjR-gOvVc"
   },
   "source": [
    "### Inner Cross Validation\n",
    "Return Param + F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gshV5lDUrbBX"
   },
   "outputs": [],
   "source": [
    "def inner_cross_validation(n_splits, model, params, X_train, y_train, X_test, y_test):\n",
    "  # Configure the cross-validation procedure\n",
    "  # cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=10)\n",
    "  cv = StratifiedKFold(n_splits=n_splits, shuffle=False)\n",
    "\n",
    "  # Define search\n",
    "  # n_jobs = Number of job run parallel\n",
    "  # refit = retrain best estimator with whole dataset\n",
    "  search = GridSearchCV(estimator=model, param_grid=params, scoring='accuracy', cv=cv, n_jobs=-1, refit=False)\n",
    "\n",
    "  # Execute search\n",
    "  result = search.fit(X_train, y_train)\n",
    "\n",
    "  # Best Paramteter\n",
    "  best_parameter = result.best_params_\n",
    "\n",
    "  # Cross Validation DataFrame\n",
    "  cv_res = pd.DataFrame(result.cv_results_)\n",
    "  cv_res['params'] = cv_res['params'].map(lambda x: str(x)) # Convert Dictionary to String type\n",
    "  cv_res = cv_res[['params','mean_test_score']]\n",
    "\n",
    "  return cv_res, best_parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBMtEGyp696r"
   },
   "source": [
    "### K * L Fold CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLosBK2_pLvX"
   },
   "outputs": [],
   "source": [
    "def k_l_fold_cv(X, y, k_splits=3, l_splits=5, model=None, params=None):\n",
    "  if model == None:\n",
    "    print(\"Model is undefined.\")\n",
    "    return\n",
    "  if params == None:\n",
    "    print(\"Parameters is undefined.\")\n",
    "    return\n",
    "\n",
    "  # Configure the cross-validation procedure\n",
    "  # cv_outer = StratifiedKFold(n_splits=k_splits, shuffle=True, random_state=10)\n",
    "  cv_outer = StratifiedKFold(n_splits=k_splits, shuffle=False)\n",
    "  joined_inner_cv_df = pd.DataFrame()\n",
    "  fold_counter = 0\n",
    "\n",
    "  # Outer CV Result\n",
    "  outer_confusion_matrix = {\n",
    "    'tn': [],\n",
    "    'fp': [],\n",
    "    'fn': [],\n",
    "    'tp': [],\n",
    "  }\n",
    "  trained_best_models = []\n",
    "\n",
    "  for train_ix, test_ix in cv_outer.split(X, y): # Outer Fold Split\n",
    "    # Split data\n",
    "    X_train = X[X.index.isin(train_ix)]\n",
    "    y_train = y[y.index.isin(train_ix)]\n",
    "    X_test = X[X.index.isin(test_ix)]\n",
    "    y_test = y[y.index.isin(test_ix)]\n",
    "\n",
    "    # Inner CV\n",
    "    inner_cv_df, best_parameter = inner_cross_validation(l_splits, model, params, X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # Inner CV DataFrame\n",
    "    inner_cv_df.columns = ['params',f'mean_val_score_{fold_counter+1}']\n",
    "    if joined_inner_cv_df.empty:\n",
    "      joined_inner_cv_df = inner_cv_df\n",
    "    else:\n",
    "      joined_inner_cv_df = joined_inner_cv_df.join(inner_cv_df.set_index('params'), on='params')\n",
    "\n",
    "    fold_counter += 1\n",
    "    print(f\"Inner CV Fold {fold_counter} done!\")\n",
    "\n",
    "    # Train / Test Model with Best Parameter\n",
    "    # Copy Model with Best Parameter\n",
    "    best_model = clone_model(model, safe=True)\n",
    "    best_model.set_params(**best_parameter)\n",
    "\n",
    "    # Train Model\n",
    "    best_model.fit(X_train, y_train)\n",
    "    trained_best_models.append(best_model)\n",
    "\n",
    "    # Test Model\n",
    "    prediction_prob = best_model.predict_proba(X_test)\n",
    "    for p1, p2 in prediction_prob:\n",
    "      if p1 < 0 or p1 > 1:\n",
    "        print(\"ERROR PREDICTION\")\n",
    "\n",
    "    # Prediction of Single Model\n",
    "    y_pred = np.argmax(prediction_prob, axis = 1)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    outer_confusion_matrix['tn'].append(tn)\n",
    "    outer_confusion_matrix['fp'].append(fp)\n",
    "    outer_confusion_matrix['fn'].append(fn)\n",
    "    outer_confusion_matrix['tp'].append(tp)\n",
    "\n",
    "  return outer_confusion_matrix, joined_inner_cv_df, trained_best_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1MBlb-aO7lK"
   },
   "source": [
    "## Single Model Evaluation\n",
    "Iterate each Duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXyV_qI9OKT1"
   },
   "source": [
    "### Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IOF7_6PmAKhZ"
   },
   "outputs": [],
   "source": [
    "def single_model_evaluation(folder_name, model_name, model, params, features, datasets):\n",
    "  # Configure CV\n",
    "  k_splits = 3\n",
    "  l_splits = 5\n",
    "\n",
    "  # Model Evaluation Result DataFrame (Outer CV)\n",
    "  single_model_evaluation_dict = {\n",
    "      'duration': [],\n",
    "  }\n",
    "  # Dictionary Confusion Matrix Per K Fold (Outer CV)\n",
    "  confusion_matrix_cells = ['tn', 'fp', 'fn', 'tp']\n",
    "  for confusion_matrix_cell in confusion_matrix_cells:\n",
    "    for fold_id in range(k_splits):\n",
    "      single_model_evaluation_dict[f\"{confusion_matrix_cell}_{fold_id+1}\"] = []\n",
    "\n",
    "  # Dictionary Metrics Per K Fold (Outer CV) and Aggregate Result\n",
    "  metric_names = ['accuracy','precision', 'recall', 'f1_score']\n",
    "  for metric_name in metric_names:\n",
    "    for fold_id in range(k_splits):\n",
    "      single_model_evaluation_dict[f\"{metric_name}_{fold_id+1}\"] = []\n",
    "    single_model_evaluation_dict[f\"mean_{metric_name}\"] = []\n",
    "    single_model_evaluation_dict[f\"std_{metric_name}\"] = []\n",
    "\n",
    "  for duration, dataset in enumerate(datasets):\n",
    "    time_duration = duration + 2\n",
    "    print(\"{} minute duration...\".format(time_duration))\n",
    "\n",
    "    # Feature Selection\n",
    "    columns = dataset.columns\n",
    "    X = dataset[columns[:-1]]\n",
    "    y = dataset[columns[-1]]\n",
    "    X = X[features]\n",
    "\n",
    "    # Train Test\n",
    "    outer_confusion_matrix, joined_inner_cv_df, trained_best_models = k_l_fold_cv(X=X, y=y, k_splits=k_splits, l_splits=l_splits, model=model, params=params)\n",
    "\n",
    "    # Save Inner Cross Validation Result (Parameter Combinations)\n",
    "    cv_result_filename = f\"{model_name}_duration_{time_duration}_inner_cv.csv\"\n",
    "    save_csv(df=joined_inner_cv_df, folder=folder_name, filename=cv_result_filename)\n",
    "\n",
    "    # Save Trained Models\n",
    "    for fold_id, model in enumerate(trained_best_models):\n",
    "      model_filename = f\"{model_name}_duration_{time_duration}_fold_{fold_id+1}.pickle\"\n",
    "      save_model(model=model, folder=folder_name, filename=model_filename)\n",
    "\n",
    "    # Outer CV Confusion Matrix\n",
    "    for confusion_matrix_cell in confusion_matrix_cells:\n",
    "      confusion_matrix_results = outer_confusion_matrix[confusion_matrix_cell]\n",
    "      for fold_id, confusion_matrix_result in enumerate(confusion_matrix_results): # For each Outer CV Result\n",
    "        single_model_evaluation_dict[f\"{confusion_matrix_cell}_{fold_id+1}\"].append(confusion_matrix_result)\n",
    "\n",
    "    # Configure Dictionary for All Outer CV Result\n",
    "    metric_outer_cv_results = {}\n",
    "    for metric_name in metric_names:\n",
    "      metric_outer_cv_results[metric_name] = []\n",
    "\n",
    "    # Metrics Per Fold\n",
    "    for fold_id in range(k_splits):\n",
    "      tn = single_model_evaluation_dict[f\"tn_{fold_id+1}\"][duration]\n",
    "      fp = single_model_evaluation_dict[f\"fp_{fold_id+1}\"][duration]\n",
    "      fn = single_model_evaluation_dict[f\"fn_{fold_id+1}\"][duration]\n",
    "      tp = single_model_evaluation_dict[f\"tp_{fold_id+1}\"][duration]\n",
    "\n",
    "      metric_results = get_metrics(tn, fp, fn, tp)\n",
    "      for metric_name in metric_names:\n",
    "        metric_result = metric_results[metric_name]\n",
    "        single_model_evaluation_dict[f\"{metric_name}_{fold_id+1}\"].append(metric_result)\n",
    "        metric_outer_cv_results[metric_name].append(metric_result)\n",
    "\n",
    "    # Aggregate Metrics\n",
    "    for metric_name in metric_names:\n",
    "      metric_outer_cv_result = metric_outer_cv_results[metric_name]\n",
    "      single_model_evaluation_dict[f'mean_{metric_name}'].append(np.mean(metric_outer_cv_result)) # Metrics Average\n",
    "      single_model_evaluation_dict[f'std_{metric_name}'].append(np.std(metric_outer_cv_result)) # Metrics Std\n",
    "\n",
    "    # Add Duration and Best Parameter\n",
    "    single_model_evaluation_dict['duration'].append(time_duration)\n",
    "\n",
    "  # Save Single Model Evaluation DataFrame (Outer Cross Validation)\n",
    "  single_model_df = pd.DataFrame(single_model_evaluation_dict)\n",
    "  save_csv(df=single_model_df, folder=folder_name, filename=f\"{model_name}_final_result.csv\")\n",
    "\n",
    "  return single_model_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FB8Fxn0Uo5hb"
   },
   "source": [
    "## Joo (2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAb_Ado1rwBR"
   },
   "outputs": [],
   "source": [
    "model_1 = MLPClassifier(hidden_layer_sizes=(25, 30), shuffle=False, verbose=False, random_state=6)\n",
    "model_1_features = ['MeanNN','SDNN','RMSSD','pNN50','VLF','LF','HF','LF/HF','SD1','SD2','SD1/SD2']\n",
    "\n",
    "model_1_params = dict()\n",
    "model_1_params['solver'] = ['adam', 'sgd']\n",
    "model_1_params['learning_rate_init'] = [0.001, 0.01]\n",
    "model_1_params['max_iter'] = [250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500]\n",
    "model_1_params['activation'] = ['logistic','tanh','relu']\n",
    "\n",
    "cv_result = single_model_evaluation(\"Joo\", \"Neural Network\", model_1, model_1_params, model_1_features, scaled_datasets)\n",
    "cv_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1MdZN8ZM4DA"
   },
   "source": [
    "## Lee (2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUlxJH8PUl2W"
   },
   "outputs": [],
   "source": [
    "model_6 = MLPClassifier(hidden_layer_sizes=(5), shuffle=False, verbose=False, random_state=2)\n",
    "model_6_features = ['MeanNN','SDNN','RMSSD','pNN50','VLF','LF','HF','LF/HF','SD1','SD2','SD1/SD2']\n",
    "\n",
    "model_6_params = dict()\n",
    "model_6_params['solver'] = ['adam', 'sgd']\n",
    "model_6_params['learning_rate_init'] = [0.001, 0.01]\n",
    "model_6_params['max_iter'] = [250, 500, 750, 1000, 1250, 1500, 1750, 2000, 2250, 2500]\n",
    "model_6_params['activation'] = ['logistic','tanh','relu']\n",
    "\n",
    "cv_result_6 = single_model_evaluation(\"Lee\", \"Neural Network\", model_6, model_6_params, model_6_features, scaled_datasets)\n",
    "cv_result_6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGdNeMsaslff"
   },
   "source": [
    "## Murukesan (2014)\n",
    "4. SVM\n",
    "5. PNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UCW83SbF2W4"
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "id": "6l0kT2G5F35X"
   },
   "outputs": [],
   "source": [
    "model_4 = SVM(kernel='rbf', gamma=1.0, probability=True, random_state=8)\n",
    "model_4_features = ['Outlier','sdHR','aTotal','pVLF','pLF','SD1','Alpha']\n",
    "\n",
    "model_4_params = dict()\n",
    "model_4_params['C'] = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\n",
    "cv_result_4 = single_model_evaluation(\"Murukesan 1\", \"SVM\", model_4, model_4_params, model_4_features, scaled_datasets)\n",
    "cv_result_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4xdh8k0N145n"
   },
   "source": [
    "### PNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RlOu6FXEisk6"
   },
   "outputs": [],
   "source": [
    "model_5 = PNN(std=0.4, verbose=False)\n",
    "model_5_features = ['Outlier','sdHR','aTotal','pVLF','pLF','SD1','Alpha']\n",
    "\n",
    "model_5_params = dict()\n",
    "model_5_params['std'] = [0.4]\n",
    "\n",
    "cv_result_5 = single_model_evaluation(\"Murukesan 2\", \"PNN\", model_5, model_5_params, model_5_features, scaled_datasets)\n",
    "cv_result_5"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "vOWpYTQrUsDs"
   ],
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
